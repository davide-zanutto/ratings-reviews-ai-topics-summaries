{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Vocabulary: ['Absorbency', 'Accessibility', 'Accessories', 'Ambience', 'App', 'Appearance', 'Arms', 'Assembly', 'Availability', 'Back', 'Battery', 'Beam', 'Bed', 'Bedside', 'Bits', 'Books', 'Brightness', 'Bulb', 'Bulbs', 'Burn time', 'Buttons', 'Cabinet', 'Car', 'Ceiling', 'Charging', 'Childproofing', 'Christmas', 'Clamps', 'Cleaning', 'Cleanliness', 'Closure', 'Cloth-like', 'Collection', 'Color', 'Colors', 'Comfort', 'Compatibility', 'Connection', 'Convenience', 'Cooking', 'Cookware', 'Cooling', 'Cord', 'Counter', 'Counter space', 'Cover', 'Covers', 'Cracks', 'Crunchiness', 'Cushion', 'Customer service', 'Customization', 'Cute', 'Decor', 'Decoration', 'Delivery', 'Design', 'Desk', 'Display', 'Door', 'Doors', 'Drainage', 'Drawer', 'Drawers', 'Durability', 'Ease of use', 'Easy to use', 'Edges', 'Effectiveness', 'Extension', 'Fabric', 'Falling', 'Features', 'Filling', 'Finish', 'Firmness', 'Fit', 'Flatness', 'Footboard', 'Frame', 'Functionality', 'Gift', 'Glass', 'Gnome', 'Greenery', 'Handle', 'Handy', 'Hanging', 'Hanging loop', 'Heat', 'Heating', 'Height', 'Hinges', 'Holiday', 'Induction', 'Infant', 'Installation', 'Instructions', 'Integration', 'Kids', 'Kitchen', 'Kitchen use', 'Knob', 'Leaks', 'Legs', 'Length', 'Light', 'Light-blocking', 'Lightness', 'Lightweight', 'Machine washable', 'Magnet', 'Marks', 'Match', 'Material', 'Mattress', 'Metal', 'Missing parts', 'Neck', 'Nightstand', 'Noise', 'Non-slip', 'Nonstick', 'Operation', 'Organization', 'Outdoor use', 'Oven', 'Parts', 'Pegboard', 'Performance', 'Pillowcase', 'Plumbing', 'Power', 'Pressure', 'Price', 'Project', 'Protection', 'Purchase', 'Quality', 'Racks', 'Reading', 'Realism', 'Refrigerator', 'Reliability', 'Remote', 'Removal', 'Replacement', 'Return', 'Reusability', 'Room', 'Rust', 'Safety', 'Salt', 'Scent', 'Scratch', 'Scratches', 'Screws', 'Seal', 'Service', 'Set', 'Settings', 'Setup', 'Sewing', 'Shape', 'Shedding', 'Shelf', 'Shelves', 'Shelving', 'Size', 'Slats', 'Slipperiness', 'Smell', 'Softness', 'Sound', 'Space', 'Stability', 'Staff', 'Stainless steel', 'Stem', 'Storage', 'Strap', 'Strength', 'Sturdiness', 'Sturdy', 'Style', 'Support', 'Surface', 'Switch', 'Sync', 'Taste', 'Temperature', 'Template', 'Thickness', 'Timer', 'Toddler', 'Towel rack', 'Trofast', 'Usability', 'Usefulness', 'Utensils', 'Value', 'Vase', 'Ventilation', 'Versatility', 'Warmth', 'Warranty', 'Washability', 'Washable', 'Washing', 'Water', 'Weather', 'Weight', 'Wheels', 'Wifi', 'Wobbling', 'Wood']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10081/10081 [00:00<00:00, 14769.16 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1121/1121 [00:00<00:00, 15796.08 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10081/10081 [00:00<00:00, 15744.90 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1121/1121 [00:00<00:00, 14891.88 examples/s]\n",
      "Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10081/10081 [00:00<00:00, 935935.95 examples/s]\n",
      "Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1121/1121 [00:00<00:00, 481151.74 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/davide.zanutto1/Desktop/ratings-reviews-ai-summaries-topics/ratings-reviews/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/yb/zpvj9lnd4130b8bpn770jb680000gp/T/ipykernel_18522/2258035979.py:126: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3783' max='3783' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3783/3783 17:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.041415</td>\n",
       "      <td>0.323024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.039163</td>\n",
       "      <td>0.314220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.037591</td>\n",
       "      <td>0.327408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='141' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [141/141 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.03759075701236725, 'eval_f1': 0.32740825688073394, 'eval_runtime': 11.4674, 'eval_samples_per_second': 97.755, 'eval_steps_per_second': 12.296, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from datasets import Dataset, Features, Sequence, Value\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ==================================================\n",
    "# 1. Load CSV and Preprocess Input Data\n",
    "# ==================================================\n",
    "# Assumes your CSV file has columns: article_id, review, all_topics, selected_topics\n",
    "csv_file = \"csv/GroundTruthProdArea10k.csv\"  # Change this to the path of your CSV file.\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Convert the string representations of lists into actual lists.\n",
    "df[\"all_topics\"] = df[\"all_topics\"].apply(ast.literal_eval)\n",
    "df[\"selected_topics\"] = df[\"selected_topics\"].apply(ast.literal_eval)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Build a Global Vocabulary of Candidate Topics\n",
    "# --------------------------------------------------\n",
    "global_vocab = sorted(set.union(*df[\"all_topics\"].apply(set)))\n",
    "print(\"Global Vocabulary:\", global_vocab)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Create a Binary Label Vector for Each Example\n",
    "# --------------------------------------------------\n",
    "def create_label_vector(selected_topics, vocab):\n",
    "    # 1 if topic is selected, 0 otherwise.\n",
    "    return [1 if topic in selected_topics else 0 for topic in vocab]\n",
    "\n",
    "df[\"labels\"] = df[\"selected_topics\"].apply(lambda st: create_label_vector(st, global_vocab))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Create the Input Text With Candidate Topics\n",
    "# --------------------------------------------------\n",
    "def create_input_text(row):\n",
    "    candidates = \", \".join(row[\"all_topics\"])\n",
    "    review_text = row[\"review\"]\n",
    "    return f\"Candidate topics: {candidates}. Review: {review_text}\"\n",
    "\n",
    "df[\"input_text\"] = df.apply(create_input_text, axis=1)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Select Only the Columns Needed for Model Training\n",
    "# --------------------------------------------------\n",
    "df_processed = df[[\"input_text\", \"labels\"]]\n",
    "\n",
    "# ==================================================\n",
    "# 2. Convert the DataFrame to a Hugging Face Dataset\n",
    "# ==================================================\n",
    "dataset = Dataset.from_pandas(df_processed)\n",
    "\n",
    "# Optionally perform a train/test split (e.g., 80/20 split):\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# ==================================================\n",
    "# 3. Tokenize the Data for the Model\n",
    "# ==================================================\n",
    "model_name = \"bert-base-uncased\"  # Change to \"roberta-base\" or \"microsoft/deberta-base\" if desired.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"input_text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# Remove the raw text column since itâ€™s not needed anymore.\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"input_text\"])\n",
    "\n",
    "# ==================================================\n",
    "# 4. Convert the \"labels\" Column to Floats\n",
    "# ==================================================\n",
    "# Force each \"labels\" list to have float values.\n",
    "def convert_labels(example):\n",
    "    example[\"labels\"] = [float(x) for x in example[\"labels\"]]\n",
    "    return example\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(convert_labels)\n",
    "\n",
    "# Now, we explicitly cast the \"labels\" column to be a Sequence of float32.\n",
    "# This prevents the DataCollator from automatically converting them back to int.\n",
    "new_features = tokenized_datasets[\"train\"].features.copy()\n",
    "new_features[\"labels\"] = Sequence(feature=Value(\"float32\"))\n",
    "tokenized_datasets = tokenized_datasets.cast(new_features)\n",
    "\n",
    "# Set the dataset format to PyTorch tensors.\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# ==================================================\n",
    "# 5. Define a Metrics Function for Evaluation\n",
    "# ==================================================\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Apply sigmoid to obtain probabilities.\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    # Threshold at 0.5 to obtain binary predictions.\n",
    "    predictions = (probs > 0.5).astype(int)\n",
    "    f1 = f1_score(labels, predictions, average=\"micro\")\n",
    "    return {\"f1\": f1}\n",
    "\n",
    "# ==================================================\n",
    "# 6. Load and Configure the Model for Fine-Tuning\n",
    "# ==================================================\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(global_vocab),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# ==================================================\n",
    "# 7. Set Up the Trainer and Training Arguments\n",
    "# ==================================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./fine_tuned_{model_name}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ==================================================\n",
    "# 8. Fine-Tune the Model and Evaluate\n",
    "# ==================================================\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ratings-reviews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
