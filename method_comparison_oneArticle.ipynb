{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davide.zanutto1/Desktop/ratings-reviews-ai-topics-summaries/ratings-reviews/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1016 reviews\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import logging\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "project_id = 'ingka-tugc-infra-prod'\n",
    "dataset_id = 'eu_ai_content'\n",
    "table_id = 'reviews'\n",
    "\n",
    "\n",
    "table_ref = f'{project_id}.{dataset_id}.{table_id}'\n",
    "\n",
    "# Query to get all the data - 1.17GB to process\n",
    "\n",
    "article_id = '40598766'\n",
    "\n",
    "query_all = f\"\"\"\n",
    "    SELECT concat(r.title, '. ', r.text) as review_text\n",
    "    FROM {table_ref} r\n",
    "    WHERE franchise='set-11' AND content_lang_code = 'en' AND art_id = '{article_id}'\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query_all)\n",
    "\n",
    "reviews = [row['review_text'] for row in query_job]\n",
    "print(f\"Processing {len(reviews)} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from utils.getSecret import get_secret\n",
    "\n",
    "project = \"923326131319\"\n",
    "secret  = \"derai-azure\"\n",
    "api_key = get_secret(project, secret)\n",
    "\n",
    "llm_client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-07-01-preview\",\n",
    "    azure_endpoint=\"https://derai-vision.openai.azure.com/\",\n",
    ")\n",
    "\n",
    "model = \"gpt-4o\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generateTopics import get_topics\n",
    "\n",
    "topics = get_topics(reviews, llm_client, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of deleted topics: 0\n",
      "Unique topics: ['Quality', 'Price', 'Color', 'Fabric', 'Value', 'Size', 'Washability', 'Zipper']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Encode the topics\n",
    "topic_embeddings = model.encode(topics, convert_to_numpy=True)\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(topic_embeddings)\n",
    "\n",
    "# Set a similarity threshold\n",
    "similarity_threshold = 0.8\n",
    "\n",
    "# Identify and remove similar topics\n",
    "unique_topics = topics.copy()\n",
    "for i in range(len(topics)):\n",
    "    for j in range(i + 1, len(topics)):\n",
    "        if similarity_matrix[i, j] > similarity_threshold:\n",
    "            if topics[j] in unique_topics:\n",
    "                unique_topics.remove(topics[j])\n",
    "                \n",
    "deleted_topics_count = len(topics) - len(unique_topics)\n",
    "print(\"Number of deleted topics:\", deleted_topics_count)\n",
    "print(\"Unique topics:\", unique_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Identification Evaluation - Are those 8 topics good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"Quality\": 4,\n",
      "  \"Price\": 5,\n",
      "  \"Color\": 5,\n",
      "  \"Fabric\": 4,\n",
      "  \"Value\": 5,\n",
      "  \"Size\": 3,\n",
      "  \"Washability\": 4,\n",
      "  \"Zipper\": 3\n",
      "}\n",
      "```\n",
      "\n",
      "| Topic       | Rating | Justification                                                                                      |\n",
      "|-------------|--------|----------------------------------------------------------------------------------------------------|\n",
      "| Quality     | 4      | Frequently mentioned with both positive and negative comments, indicating high relevance.          |\n",
      "| Price       | 5      | Dominantly discussed as a key factor, mostly positively, making it highly relevant.                |\n",
      "| Color       | 5      | Extensively discussed in terms of variety, accuracy, and impact on decor.                         |\n",
      "| Fabric      | 4      | Regularly mentioned regarding texture, durability, and material quality.                          |\n",
      "| Value       | 5      | Often highlighted as excellent or great, closely tied with price and quality.                      |\n",
      "| Size        | 3      | Some reviews address size fit, but less frequently than other topics.                             |\n",
      "| Washability | 4      | Commonly discussed with positive and negative aspects of washing and maintenance.                 |\n",
      "| Zipper      | 3      | Mentioned occasionally with both praise and issues, showing moderate relevance.                   |\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from openai import AzureOpenAI\n",
    "prompt = fYou are an AI review analyst. Your task is to analyze a collection of reviews and assess the following topics: \"{unique_topics}\" \n",
    "            Go through all of the reviews and check that each topic is mentioned either explicitly or implicitly. \n",
    "            Once you read all the reviews, provide a rating for each topic (from 0 to 5) considering the relevance of the topic to the reviews.\n",
    "            If a topic is not mentioned in the reviews, you should rate it as 0. If it is mentioned in a lot of reviews, you should rate it as 5.\n",
    "            Reviews: \"{', '.join(reviews)}\" \n",
    "            Answer first with an array of scoring of the type 'Topic':'Rating' (e.g.: Quality: 5, Price: 5, ...) and after that, on a new line, \n",
    "            with a table containing the topics, their ratings and a brief justification on why that score has been assigned. \n",
    "            Remember that the score only indicated how much a topic is relevant for that group of reviews. \n",
    "            Do not add any additional text to the answer.\n",
    "\n",
    "ai_client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-07-01-preview\",\n",
    "    azure_endpoint=\"https://derai-vision.openai.azure.com/\",\n",
    ")\n",
    "result = ai_client.chat.completions.create(\n",
    "    model=\"o1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(result.choices[0].message.content)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic assignment - Labeling each review with 0 to N topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deberta-v3-base-finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning topics to reviews...\n",
      "Results saved to deberta_finetuned.csv, time taken: 46.330167055130005\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_dir = 'artifacts/deberta-v3-base-finetuned:v11'  \n",
    "\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "from utils.assignTopics import get_reviews_labels_deBERTa\n",
    "\n",
    "print(\"Assigning topics to reviews...\")\n",
    "results = []\n",
    "for review in reviews:\n",
    "    try:\n",
    "        assigned = get_reviews_labels_deBERTa(tokenizer, model, device, review, topics, threshold=0.3)\n",
    "        results.append([review, assigned, 1])\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if \"content_filter\" in msg or \"ResponsibleAIPolicyViolation\" in msg:\n",
    "            logging.error(f\"Content filter triggered for review: {review} – skipping.\")\n",
    "        else:\n",
    "            logging.error(f\"Error processing review: {review} – {e}\")\n",
    "        results.append([review, [], 1])\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\"sequence\", \"labels\", \"scores\"])\n",
    "\n",
    "# Save to CSV\n",
    "df_results.to_csv(\"csv/deberta_finetuned.csv\", index=False)\n",
    "\n",
    "deberta_finetuned_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to deberta_finetuned.csv, time taken:\", deberta_finetuned_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deberta-pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning topics to reviews...\n",
      "Results saved to deberta_pairwise_finetuned.csv, time taken: 123.74009490013123\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_dir = 'artifacts/deberta-v3-pairwise-finetuned:v1'  \n",
    "\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "from utils.assignTopics import get_review_labels_deBERTa_pairwise\n",
    "\n",
    "print(\"Assigning topics to reviews...\")\n",
    "results = []\n",
    "for review in reviews:\n",
    "    try:\n",
    "        assigned = get_review_labels_deBERTa_pairwise(tokenizer, model, device, review, topics, threshold=0.5)\n",
    "        results.append([review, assigned, 1])\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if \"content_filter\" in msg or \"ResponsibleAIPolicyViolation\" in msg:\n",
    "            logging.error(f\"Content filter triggered for review: {review} – skipping.\")\n",
    "        else:\n",
    "            logging.error(f\"Error processing review: {review} – {e}\")\n",
    "        results.append([review, [], 1])\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\"sequence\", \"labels\", \"scores\"])\n",
    "\n",
    "# Save to CSV\n",
    "df_results.to_csv(\"csv/deberta_pairwise_finetuned.csv\", index=False)\n",
    "\n",
    "deberta_pairwise_finetuned_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to deberta_pairwise_finetuned.csv, time taken:\", deberta_pairwise_finetuned_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nli-deberta-v3-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to nli-deberta-v3.csv, time taken: 177.73452305793762\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/nli-deberta-v3-base\", use_fast=False)\n",
    "classifier = pipeline(\"zero-shot-classification\", model='cross-encoder/nli-deberta-v3-base', tokenizer=tokenizer)\n",
    "\n",
    "results = []\n",
    "for review in reviews:\n",
    "    result = classifier(review, topics, multi_label=True)\n",
    "    results.append(result)\n",
    "\n",
    "# Transform results into a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_results.to_csv('csv/nli-deberta-v3.csv', index=False)\n",
    "\n",
    "nlidebertav3_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to nli-deberta-v3.csv, time taken:\", nlidebertav3_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bart-large-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to bart-large-mnli.csv, time taken: 178.6922528743744\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "results = []\n",
    "for review in reviews:\n",
    "    result = classifier(review, topics, multi_label=True)\n",
    "    results.append(result)\n",
    "\n",
    "# Transform results into a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_results.to_csv('csv/bart-large-mnli.csv', index=False)\n",
    "\n",
    "bartlargemnli_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to bart-large-mnli.csv, time taken:\", bartlargemnli_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flan-T5-base-finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning topics to reviews...\n",
      "Results saved to flan_t5_finetuned.csv, time taken: 284.03163599967957\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model_dir = 'artifacts/flan-t5-base-finetuned:v9'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load tokenizer & Flan-T5 for classification\n",
    "# -----------------------------------------------------------------------------\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model     = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Assign topics to each review via Flan-T5\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from utils.assignTopics import get_reviews_labels_flanT5\n",
    "\n",
    "print(\"Assigning topics to reviews...\")\n",
    "results = []\n",
    "for review in reviews:\n",
    "    try:\n",
    "        assigned = get_reviews_labels_flanT5(\n",
    "            tokenizer, model, device, review, topics\n",
    "        )\n",
    "        results.append([review, assigned, 1])\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if \"content_filter\" in msg or \"ResponsibleAIPolicyViolation\" in msg:\n",
    "            logging.error(f\"Content filter triggered for review: {review} – skipping.\")\n",
    "        else:\n",
    "            logging.error(f\"Error processing review: {review} – {e}\")\n",
    "        results.append([review, [], 1])\n",
    "\n",
    "\n",
    "# Transform results into a DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"sequence\", \"labels\", \"scores\"])\n",
    "\n",
    "# Check and split labels if they contain ';'\n",
    "def split_semicolon_elements(label_list):\n",
    "    \"\"\"\n",
    "    Given a list of strings, split any element containing ';'\n",
    "    into multiple pieces, strip whitespace, and return the flat list.\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    for elem in label_list:\n",
    "        # only split strings that contain a semicolon\n",
    "        if isinstance(elem, str) and ';' in elem:\n",
    "            parts = [part.strip() for part in elem.split(';')]\n",
    "            new_labels.extend(parts)\n",
    "        else:\n",
    "            # leave other elements untouched (but strip surrounding spaces)\n",
    "            new_labels.append(elem.strip() if isinstance(elem, str) else elem)\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "# Check and split labels if they contain ';'\n",
    "df_results['labels'] = df_results['labels'].apply(split_semicolon_elements)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_results.to_csv('csv/flan_t5_finetuned.csv', index=False)\n",
    "\n",
    "flan_t5_finetuned_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to flan_t5_finetuned.csv, time taken:\", flan_t5_finetuned_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flan-T5-small-finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning topics to reviews...\n",
      "Results saved to flan_t5_small_finetuned.csv, time taken: 87.50752091407776\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_dir = 'artifacts/flan-t5-small-finetuned:v8'  \n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load tokenizer & Flan-T5 for classification\n",
    "# -----------------------------------------------------------------------------\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model     = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Assign topics to each review via Flan-T5\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from utils.assignTopics import get_reviews_labels_flanT5\n",
    "\n",
    "print(\"Assigning topics to reviews...\")\n",
    "results = []\n",
    "for review in reviews:\n",
    "    try:\n",
    "        assigned = get_reviews_labels_flanT5(\n",
    "            tokenizer, model, device, review, topics\n",
    "        )\n",
    "        results.append([review, assigned, 1])\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if \"content_filter\" in msg or \"ResponsibleAIPolicyViolation\" in msg:\n",
    "            logging.error(f\"Content filter triggered for review: {review} – skipping.\")\n",
    "        else:\n",
    "            logging.error(f\"Error processing review: {review} – {e}\")\n",
    "        results.append([review, [], 1])\n",
    "\n",
    "\n",
    "# Transform results into a DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"sequence\", \"labels\", \"scores\"])\n",
    "\n",
    "# 3) Apply it to the DataFrame\n",
    "df_results['labels'] = df_results['labels'].apply(split_semicolon_elements)\n",
    "\n",
    "df_results.to_csv('csv/flan_t5_small_finetuned.csv', index=False)\n",
    "\n",
    "flan_t5_small_finetuned_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to flan_t5_small_finetuned.csv, time taken:\", flan_t5_small_finetuned_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Zero-shot - GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to LLM.csv, time taken: 521.2778120040894\n"
     ]
    }
   ],
   "source": [
    "from utils.assignTopics import get_reviews_labels_LLM_0shot\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "results = []\n",
    "for review in reviews:\n",
    "    result = get_reviews_labels_LLM_0shot(review, topics, llm_client, \"gpt-4o-mini\")\n",
    "    results.append([review, result, 1])\n",
    "\n",
    "# Transform results into a DataFrame\n",
    "df = pd.DataFrame(results, columns=[\"sequence\", \"labels\", \"scores\"])\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('csv/LLM.csv', index=False)\n",
    "\n",
    "LLM_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to LLM.csv, time taken:\", LLM_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Json output saved!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Load the results from the three CSV files\n",
    "df_deberta_finetuned = pd.read_csv('csv/deberta_finetuned.csv')\n",
    "df_nli_deberta_v3 = pd.read_csv('csv/nli-deberta-v3.csv')\n",
    "df_deberta_pairwise_finetuned = pd.read_csv('csv/deberta_pairwise_finetuned.csv')\n",
    "df_bart_large_mnli = pd.read_csv('csv/bart-large-mnli.csv')\n",
    "df_flan_t5_finetuned = pd.read_csv('csv/flan_t5_finetuned.csv')\n",
    "df_flan_t5_small_finetuned = pd.read_csv('csv/flan_t5_small_finetuned.csv')\n",
    "llm = pd.read_csv('csv/LLM.csv')\n",
    "\n",
    "# Initialize a dictionary to hold the reviews and their corresponding topics\n",
    "reviews_topics = []\n",
    "# Add the topics to the dictionary\n",
    "reviews_topics.append({\"Identified topics\": topics})\n",
    "# Add the time taken by each method\n",
    "time_taken = {\n",
    "    \"deberta_finetuned_time\": deberta_finetuned_time,\n",
    "    \"nli_deberta_v3_time\": nlidebertav3_time,\n",
    "    \"deberta_pairwise_finetuned_time\": deberta_pairwise_finetuned_time,\n",
    "    \"bart_large_mnli_time\": bartlargemnli_time,\n",
    "    \"flan_t5_finetuned_time\": flan_t5_finetuned_time,\n",
    "    \"flan_t5_small_finetuned_time\": flan_t5_small_finetuned_time,\n",
    "    \"LLM_time\": LLM_time}\n",
    "reviews_topics.append({\"Time taken\": time_taken})\n",
    "for i, review in enumerate(reviews):\n",
    "    review_entry = {\n",
    "        \"review\": review,\n",
    "        \"deberta_finetuned_topics\": {\n",
    "            label: 1 for label in  eval(df_deberta_finetuned.iloc[i]['labels'])\n",
    "        },\n",
    "        \"nli_deberta_v3_topics\": {\n",
    "            label: score for label, score in zip(eval(df_nli_deberta_v3.iloc[i]['labels']), eval(df_nli_deberta_v3.iloc[i]['scores'])) if score > 0.9\n",
    "        },\n",
    "        \"deberta_pairwise_finetuned_topics\": {\n",
    "            label: 1 for label in  eval(df_deberta_pairwise_finetuned.iloc[i]['labels'])        \n",
    "        },\n",
    "        \"bart_large_mnli_topics\": {\n",
    "            label: score for label, score in zip(eval(df_bart_large_mnli.iloc[i]['labels']), eval(df_bart_large_mnli.iloc[i]['scores'])) if score > 0.9\n",
    "        },\n",
    "        \"flan_t5_finetuned_topics\": {\n",
    "            label: 1 for label in  eval(df_flan_t5_finetuned.iloc[i]['labels'])        \n",
    "        },\n",
    "        \"flan_t5_small_finetuned_topics\": {\n",
    "            label: 1 for label in  eval(df_flan_t5_small_finetuned.iloc[i]['labels'])        \n",
    "        },\n",
    "        \"LLM_topics\": {\n",
    "            label: 1 for label in  eval(llm.iloc[i]['labels'])\n",
    "        }   \n",
    "    }\n",
    "    reviews_topics.append(review_entry)\n",
    "\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "json_name = f'outputs/comparison_output_{ct}.json'\n",
    "# Write the dictionary to a JSON file\n",
    "with open(json_name, 'w') as json_file:\n",
    "    json.dump(reviews_topics, json_file, indent=4)\n",
    "\n",
    "print(\"Json output saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta_finetuned     Precision: 0.652  Recall: 0.827  F1: 0.729\n",
      "nli_deberta_v3        Precision: 0.393  Recall: 0.820  F1: 0.532\n",
      "deberta_pairwise_finetuned  Precision: 0.702  Recall: 0.934  F1: 0.802\n",
      "bart_large_mnli       Precision: 0.515  Recall: 0.555  F1: 0.534\n",
      "flan_t5_finetuned     Precision: 0.683  Recall: 0.832  F1: 0.750\n",
      "flan_t5_small_finetuned  Precision: 0.657  Recall: 0.825  F1: 0.731\n",
      "LLM                   Precision: 0.649  Recall: 0.921  F1: 0.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davide.zanutto1/Desktop/ratings-reviews-ai-topics-summaries/ratings-reviews/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['decor', 'delivery', 'fit'] will be ignored\n",
      "  warnings.warn(\n",
      "/Users/davide.zanutto1/Desktop/ratings-reviews-ai-topics-summaries/ratings-reviews/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['colorwashability'] will be ignored\n",
      "  warnings.warn(\n",
      "/Users/davide.zanutto1/Desktop/ratings-reviews-ai-topics-summaries/ratings-reviews/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['fit', 'matches', 'qualityprice'] will be ignored\n",
      "  warnings.warn(\n",
      "/Users/davide.zanutto1/Desktop/ratings-reviews-ai-topics-summaries/ratings-reviews/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['comfort', 'delivery', 'design', 'fit', 'style', 'variety'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def normalize_label(label: str) -> str:\n",
    "    return label.strip().lower().replace(\" \", \"\")\n",
    "\n",
    "# 1. Load the JSON (skip the first two metadata entries)\n",
    "with open(json_name, 'r') as f:\n",
    "    data = json.load(f)\n",
    "entries = data[2:]\n",
    "\n",
    "# 2. Build a mapping from review text → prediction‐entry\n",
    "review_to_entry = { e['review']: e for e in entries }\n",
    "\n",
    "# 3. Load ground‐truth, parse lists\n",
    "df_truth = pd.read_csv('csv/ground_truth_40598766.csv')\n",
    "df_truth['topics'] = df_truth['topics'].apply(ast.literal_eval)\n",
    "\n",
    "# 4. Prepare containers\n",
    "y_true, model_preds = [], {\n",
    "    'deberta_finetuned': [],\n",
    "    'nli_deberta_v3':    [],\n",
    "    'deberta_pairwise_finetuned': [],\n",
    "    'bart_large_mnli':   [],\n",
    "    'flan_t5_finetuned': [],\n",
    "    'flan_t5_small_finetuned': [],\n",
    "    'LLM':               []\n",
    "}\n",
    "\n",
    "# 5. Align on review text\n",
    "missing = []\n",
    "for _, row in df_truth.iterrows():\n",
    "    review = row['review']\n",
    "    if review not in review_to_entry:\n",
    "        missing.append(review)\n",
    "        continue\n",
    "    \n",
    "    entry = review_to_entry[review]\n",
    "    # normalize & collect ground truth\n",
    "    true_norm = [normalize_label(t) for t in row['topics']]\n",
    "    y_true.append(true_norm)\n",
    "    \n",
    "    # collect each model’s normalized predictions\n",
    "    for model_key, json_key in [\n",
    "        ('deberta_finetuned','deberta_finetuned_topics'),\n",
    "        ('nli_deberta_v3',   'nli_deberta_v3_topics'),\n",
    "        ('deberta_pairwise_finetuned','deberta_pairwise_finetuned_topics'),\n",
    "        ('bart_large_mnli',  'bart_large_mnli_topics'),\n",
    "        ('flan_t5_finetuned','flan_t5_finetuned_topics'),\n",
    "        ('flan_t5_small_finetuned','flan_t5_small_finetuned_topics'),\n",
    "        ('LLM','LLM_topics')\n",
    "    ]:\n",
    "        preds = list(entry[json_key].keys())\n",
    "        model_preds[model_key].append([normalize_label(t) for t in preds])\n",
    "\n",
    "if missing:\n",
    "    print(\"WARNING: no JSON entry for these reviews:\\n\", \"\\n\".join(missing))\n",
    "\n",
    "# 6. Binarize all labels against the full identified topic set\n",
    "identified = data[0]['Identified topics']\n",
    "norm_identified = [normalize_label(t) for t in identified]\n",
    "mlb = MultiLabelBinarizer(classes=norm_identified)\n",
    "y_true_bin = mlb.fit_transform(y_true)\n",
    "\n",
    "# 7. Compute metrics\n",
    "for model, pred_lists in model_preds.items():\n",
    "    y_pred_bin = mlb.transform(pred_lists)\n",
    "    p = precision_score(y_true_bin, y_pred_bin, average='micro', zero_division=0)\n",
    "    r = recall_score   (y_true_bin, y_pred_bin, average='micro', zero_division=0)\n",
    "    f = f1_score       (y_true_bin, y_pred_bin, average='micro', zero_division=0)\n",
    "    print(f\"{model:20s}  Precision: {p:.3f}  Recall: {r:.3f}  F1: {f:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for each model:\n",
      "deberta_finetuned: 46.33 seconds\n",
      "nli_deberta_v3: 177.73 seconds\n",
      "deberta_pairwise_finetuned: 123.74 seconds\n",
      "bart_large_mnli: 178.69 seconds\n",
      "flan_t5_finetuned: 284.03 seconds\n",
      "flan_t5_small_finetuned: 87.51 seconds\n",
      "LLM: 521.28 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken for each model:\")\n",
    "print(f\"deberta_finetuned: {deberta_finetuned_time:.2f} seconds\")\n",
    "print(f\"nli_deberta_v3: {nlidebertav3_time:.2f} seconds\")\n",
    "print(f\"deberta_pairwise_finetuned: {deberta_pairwise_finetuned_time:.2f} seconds\")\n",
    "print(f\"bart_large_mnli: {bartlargemnli_time:.2f} seconds\")\n",
    "print(f\"flan_t5_finetuned: {flan_t5_finetuned_time:.2f} seconds\")\n",
    "print(f\"flan_t5_small_finetuned: {flan_t5_small_finetuned_time:.2f} seconds\")\n",
    "print(f\"LLM: {LLM_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ratings-reviews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
