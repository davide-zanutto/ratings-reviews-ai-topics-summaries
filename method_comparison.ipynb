{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1016 reviews\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "project_id = 'ingka-tugc-infra-prod'\n",
    "dataset_id = 'eu_ai_content'\n",
    "table_id = 'reviews'\n",
    "\n",
    "\n",
    "table_ref = f'{project_id}.{dataset_id}.{table_id}'\n",
    "\n",
    "# Query to get all the data - 1.17GB to process\n",
    "\n",
    "article_id = '40598766'\n",
    "\n",
    "query_all = f\"\"\"\n",
    "    SELECT concat(r.title, '. ', r.text) as review_text\n",
    "    FROM {table_ref} r\n",
    "    WHERE franchise='set-11' AND content_lang_code = 'en' AND art_id = '{article_id}'\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query_all)\n",
    "\n",
    "reviews = [row['review_text'] for row in query_job]\n",
    "print(f\"Processing {len(reviews)} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from utils.getSecret import get_secret\n",
    "\n",
    "project = \"923326131319\"\n",
    "secret  = \"derai-azure\"\n",
    "api_key = get_secret(project, secret)\n",
    "\n",
    "llm_client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-07-01-preview\",\n",
    "    azure_endpoint=\"https://derai-vision.openai.azure.com/\",\n",
    ")\n",
    "\n",
    "model = \"gpt-4o\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generateTopics import get_topics\n",
    "\n",
    "topics = get_topics(reviews, llm_client, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davide.zanutto1/Desktop/ratings-reviews-ai-summaries-topics/ratings-reviews/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of deleted topics: 0\n",
      "Unique topics: ['Color', 'Quality', 'Price', 'Fabric', 'Delivery', 'Fit', 'Washability', 'Decor']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Define the embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Encode the topics\n",
    "topic_embeddings = model.encode(topics, convert_to_numpy=True)\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(topic_embeddings)\n",
    "\n",
    "# Set a similarity threshold\n",
    "similarity_threshold = 0.8\n",
    "\n",
    "# Identify and remove similar topics\n",
    "unique_topics = topics.copy()\n",
    "for i in range(len(topics)):\n",
    "    for j in range(i + 1, len(topics)):\n",
    "        if similarity_matrix[i, j] > similarity_threshold:\n",
    "            if topics[j] in unique_topics:\n",
    "                unique_topics.remove(topics[j])\n",
    "                \n",
    "deleted_topics_count = len(topics) - len(unique_topics)\n",
    "print(\"Number of deleted topics:\", deleted_topics_count)\n",
    "print(\"Unique topics:\", unique_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Identification Evaluation - Are those 8 topics good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "Color: 5, Quality: 5, Price: 5, Fabric: 4, Delivery: 3, Fit: 3, Washability: 5, Decor: 5\n",
      "\n",
      "| Topic      | Rating | Justification                                                   |\n",
      "|------------|--------|-----------------------------------------------------------------|\n",
      "| Color      | 5      | Mentioned frequently and prominently in reviews                |\n",
      "| Quality    | 5      | Consistently discussed with high relevance                      |\n",
      "| Price      | 5      | Frequently mentioned as a key factor                            |\n",
      "| Fabric     | 4      | Often discussed, but slightly less emphasis compared to others  |\n",
      "| Delivery   | 3      | Mentioned several times, moderate relevance                     |\n",
      "| Fit        | 3      | Mentioned occasionally, relevant but not dominant               |\n",
      "| Washability| 5      | Highly emphasized in many reviews                               |\n",
      "| Decor      | 5      | Frequently discussed in context of matching or enhancing decor   |\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "prompt = f\"\"\"You are an AI review analyst. Your task is to analyze a collection of reviews and assess the following topics: \"{unique_topics}\" \n",
    "            Go through all of the reviews and check that each topic is mentioned either explicitly or implicitly. \n",
    "            Once you read all the reviews, provide a rating for each topic (from 0 to 5) considering the relevance of the topic to the reviews.\n",
    "            If a topic is not mentioned in the reviews, you should rate it as 0. If it is mentioned in a lot of reviews, you should rate it as 5.\n",
    "            Reviews: \"{', '.join(reviews)}\" \n",
    "            Answer first with an array of scoring of the type 'Topic':'Rating' (e.g.: Quality: 5, Price: 5, ...) and after that, on a new line, \n",
    "            with a table containing the topics, their ratings and a brief justification on why that score has been assigned. \n",
    "            Remember that the score only indicated how much a topic is relevant for that group of reviews. \n",
    "            Do not add any additional text to the answer.\"\"\"\n",
    "\n",
    "ai_client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-07-01-preview\",\n",
    "    azure_endpoint=\"https://derai-vision.openai.azure.com/\",\n",
    ")\n",
    "result = ai_client.chat.completions.create(\n",
    "    model=\"o1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic assignment - Labeling each review with 0 to N topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deberta-v3-base-finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning topics to reviews...\n",
      "Results saved to deberta_finetuned.csv, time taken: 52.4697151184082\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import logging\n",
    "\n",
    "model_dir = 'artifacts/deberta-v3-base-finetuned:v11'  \n",
    "\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "from utils.assignTopics import get_reviews_labels_deBERTa\n",
    "\n",
    "print(\"Assigning topics to reviews...\")\n",
    "results = []\n",
    "for review in reviews:\n",
    "    try:\n",
    "        assigned = get_reviews_labels_deBERTa(tokenizer, model, device, review, topics, threshold=0.3)\n",
    "        results.append([review, assigned, 1])\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if \"content_filter\" in msg or \"ResponsibleAIPolicyViolation\" in msg:\n",
    "            logging.error(f\"Content filter triggered for review: {review} – skipping.\")\n",
    "        else:\n",
    "            logging.error(f\"Error processing review: {review} – {e}\")\n",
    "        results.append([review, [], 1])\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\"sequence\", \"labels\", \"scores\"])\n",
    "\n",
    "# Save to CSV\n",
    "df_results.to_csv(\"csv/deberta_finetuned.csv\", index=False)\n",
    "\n",
    "deberta_finetuned_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to deberta_finetuned.csv, time taken:\", deberta_finetuned_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deberta-pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'artifacts/deberta-v3-pairwise-finetuned:v1'  \n",
    "\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "from utils.assignTopics import get_reviews_labels_deBERTa\n",
    "\n",
    "print(\"Assigning topics to reviews...\")\n",
    "results = []\n",
    "for review in reviews:\n",
    "    try:\n",
    "        assigned = get_reviews_labels_deBERTa(tokenizer, model, device, review, topics, threshold=0.3)\n",
    "        results.append([review, assigned, 1])\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if \"content_filter\" in msg or \"ResponsibleAIPolicyViolation\" in msg:\n",
    "            logging.error(f\"Content filter triggered for review: {review} – skipping.\")\n",
    "        else:\n",
    "            logging.error(f\"Error processing review: {review} – {e}\")\n",
    "        results.append([review, [], 1])\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\"sequence\", \"labels\", \"scores\"])\n",
    "\n",
    "# Save to CSV\n",
    "df_results.to_csv(\"csv/deberta_finetuned.csv\", index=False)\n",
    "\n",
    "deberta_finetuned_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to deberta_finetuned.csv, time taken:\", deberta_finetuned_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nli-deberta-v3-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to nli-deberta-v3.csv, time taken: 190.302903175354\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/nli-deberta-v3-base\", use_fast=False)\n",
    "classifier = pipeline(\"zero-shot-classification\", model='cross-encoder/nli-deberta-v3-base', tokenizer=tokenizer)\n",
    "\n",
    "results = []\n",
    "for review in reviews:\n",
    "    result = classifier(review, topics, multi_label=True)\n",
    "    results.append(result)\n",
    "\n",
    "# Transform results into a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_results.to_csv('csv/nli-deberta-v3.csv', index=False)\n",
    "\n",
    "nlidebertav3_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to nli-deberta-v3.csv, time taken:\", nlidebertav3_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bart-large-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to bart-large-mnli.csv, time taken: 194.8539719581604\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "results = []\n",
    "for review in reviews:\n",
    "    result = classifier(review, topics, multi_label=True)\n",
    "    results.append(result)\n",
    "\n",
    "# Transform results into a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_results.to_csv('csv/bart-large-mnli.csv', index=False)\n",
    "\n",
    "bartlargemnli_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to bart-large-mnli.csv, time taken:\", bartlargemnli_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flan-T5-base-finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning topics to reviews...\n",
      "Results saved to flan_t5_finetuned.csv, time taken: 279.5624740123749\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model_dir = 'artifacts/flan-t5-base-finetuned:v9'  # adjust if your artifact lives in a subdirectory\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load tokenizer & Flan-T5 for classification\n",
    "# -----------------------------------------------------------------------------\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model     = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Assign topics to each review via Flan-T5\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "from assignTopics import get_reviews_labels_flanT5\n",
    "\n",
    "print(\"Assigning topics to reviews...\")\n",
    "results = []\n",
    "for review in reviews:\n",
    "    try:\n",
    "        assigned = get_reviews_labels_flanT5(\n",
    "            tokenizer, model, device, review, topics\n",
    "        )\n",
    "        results.append([review, assigned, 1])\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if \"content_filter\" in msg or \"ResponsibleAIPolicyViolation\" in msg:\n",
    "            logging.error(f\"Content filter triggered for review: {review} – skipping.\")\n",
    "        else:\n",
    "            logging.error(f\"Error processing review: {review} – {e}\")\n",
    "        results.append([review, [], 1])\n",
    "\n",
    "\n",
    "# Transform results into a DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"sequence\", \"labels\", \"scores\"])\n",
    "\n",
    "# Check and split labels if they contain ';'\n",
    "df_results['labels'] = df_results['labels'].apply(lambda x: x.split(';') if ';' in x else x)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_results.to_csv('csv/flan_t5_finetuned.csv', index=False)\n",
    "\n",
    "flan_t5_finetuned_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to flan_t5_finetuned.csv, time taken:\", flan_t5_finetuned_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flan-T5-small-finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavide-zanutto\u001b[0m (\u001b[33mdigital-ethics-responsible-ai\u001b[0m) to \u001b[32mhttps://wandb.mlops.ingka.com\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/davide.zanutto1/Desktop/ratings-reviews-ai-summaries-topics/wandb/run-20250430_105059-ivuz8k86</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.mlops.ingka.com/digital-ethics-responsible-ai/ratings-reviews-ai-summaries-topics/runs/ivuz8k86' target=\"_blank\">mild-salad-5</a></strong> to <a href='https://wandb.mlops.ingka.com/digital-ethics-responsible-ai/ratings-reviews-ai-summaries-topics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.mlops.ingka.com/digital-ethics-responsible-ai/ratings-reviews-ai-summaries-topics' target=\"_blank\">https://wandb.mlops.ingka.com/digital-ethics-responsible-ai/ratings-reviews-ai-summaries-topics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.mlops.ingka.com/digital-ethics-responsible-ai/ratings-reviews-ai-summaries-topics/runs/ivuz8k86' target=\"_blank\">https://wandb.mlops.ingka.com/digital-ethics-responsible-ai/ratings-reviews-ai-summaries-topics/runs/ivuz8k86</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact flan-t5-small-finetuned:v8, 1180.77MB. 20 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   20 of 20 files downloaded.  \n",
      "Done. 0:1:15.8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mild-salad-5</strong> at: <a href='https://wandb.mlops.ingka.com/digital-ethics-responsible-ai/ratings-reviews-ai-summaries-topics/runs/ivuz8k86' target=\"_blank\">https://wandb.mlops.ingka.com/digital-ethics-responsible-ai/ratings-reviews-ai-summaries-topics/runs/ivuz8k86</a><br> View project at: <a href='https://wandb.mlops.ingka.com/digital-ethics-responsible-ai/ratings-reviews-ai-summaries-topics' target=\"_blank\">https://wandb.mlops.ingka.com/digital-ethics-responsible-ai/ratings-reviews-ai-summaries-topics</a><br>Synced 8 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250430_105059-ivuz8k86/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = 'artifacts/flan-t5-small-finetuned:v8'  # adjust if your artifact lives in a subdirectory\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load tokenizer & Flan-T5 for classification\n",
    "# -----------------------------------------------------------------------------\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model     = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Assign topics to each review via Flan-T5\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "from utils.assignTopics import get_reviews_labels_flanT5\n",
    "\n",
    "print(\"Assigning topics to reviews...\")\n",
    "results = []\n",
    "for review in reviews:\n",
    "    try:\n",
    "        assigned = get_reviews_labels_flanT5(\n",
    "            tokenizer, model, device, review, topics\n",
    "        )\n",
    "        results.append([review, assigned, 1])\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if \"content_filter\" in msg or \"ResponsibleAIPolicyViolation\" in msg:\n",
    "            logging.error(f\"Content filter triggered for review: {review} – skipping.\")\n",
    "        else:\n",
    "            logging.error(f\"Error processing review: {review} – {e}\")\n",
    "        results.append([review, [], 1])\n",
    "\n",
    "\n",
    "# Transform results into a DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"sequence\", \"labels\", \"scores\"])\n",
    "\n",
    "# Check and split labels if they contain ';'\n",
    "df_results['labels'] = df_results['labels'].apply(lambda x: x.split(';') if ';' in x else x)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_results.to_csv('csv/flan_t5_small_finetuned.csv', index=False)\n",
    "\n",
    "flan_t5_small_finetuned_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to flan_t5_small_finetuned.csv, time taken:\", flan_t5_small_finetuned_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Zero-shot - GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_labels(review, topics):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful customer reviews expert that identifies the main topics in a review.\\n\"\n",
    "                \"Provide the output as a comma-separated list of topics.\\n\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Read the following review and associate the topics mentioned implicitly or explicitly in the review.\\n\"\n",
    "                \"Only answer with the topics that are mentioned in the review. Example: ['price', 'quality']. \\n\"\n",
    "                \"If you cannot identify any topics, just return '[]' \\n\"\n",
    "                \"Do not generate any new topic, just use the ones provided.\\n\"\n",
    "                f\"Review: '{review}' \\n\"\n",
    "                f\"Topics: '{topics}' \\n\"\n",
    "                f\"Topics mentioned within the review:\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = ' '\n",
    "    model = \"gpt-4o-mini\" \n",
    "    # Generate the topic word using the language model\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=30,\n",
    "        temperature=0.4,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    # Extract and return the topic word\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to LLM.csv, time taken: 448.77764892578125\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "results = []\n",
    "for review in reviews:\n",
    "    result = get_reviews_labels(review, topics)\n",
    "    results.append([review, result, 1])\n",
    "\n",
    "# Transform results into a DataFrame\n",
    "df = pd.DataFrame(results, columns=[\"sequence\", \"labels\", \"scores\"])\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('csv/LLM.csv', index=False)\n",
    "\n",
    "LLM_time = time.time() - start\n",
    "\n",
    "print(\"Results saved to LLM.csv, time taken:\", LLM_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Json output saved!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Load the results from the three CSV files\n",
    "df_deberta_finetuned = pd.read_csv('csv/deberta_finetuned.csv')\n",
    "df_nli_deberta_v3 = pd.read_csv('csv/nli-deberta-v3.csv')\n",
    "df_bart_large_mnli = pd.read_csv('csv/bart-large-mnli.csv')\n",
    "df_flan_t5_finetuned = pd.read_csv('csv/flan_t5_finetuned.csv')\n",
    "df_flan_t5_small_finetuned = pd.read_csv('csv/flan_t5_small_finetuned.csv')\n",
    "llm = pd.read_csv('csv/LLM.csv')\n",
    "\n",
    "# Initialize a dictionary to hold the reviews and their corresponding topics\n",
    "reviews_topics = []\n",
    "# Add the topics to the dictionary\n",
    "reviews_topics.append({\"Identified topics\": topics})\n",
    "# Add the time taken by each method\n",
    "time_taken = {\n",
    "    \"deberta_finetuned_time\": deberta_finetuned_time,\n",
    "    \"nli_deberta_v3_time\": nlidebertav3_time,\n",
    "    \"bart_large_mnli_time\": bartlargemnli_time,\n",
    "    \"flan_t5_finetuned_time\": flan_t5_finetuned_time,\n",
    "    \"flan_t5_small_finetuned_time\": flan_t5_small_finetuned_time,\n",
    "    \"LLM_time\": LLM_time}\n",
    "reviews_topics.append({\"Time taken\": time_taken})\n",
    "for i, review in enumerate(reviews):\n",
    "    review_entry = {\n",
    "        \"review\": review,\n",
    "        \"deberta_finetuned_topics\": {\n",
    "            label: 1 for label in  eval(df_deberta_finetuned.iloc[i]['labels'])\n",
    "        },\n",
    "        \"nli_deberta_v3_topics\": {\n",
    "            label: score for label, score in zip(eval(df_nli_deberta_v3.iloc[i]['labels']), eval(df_nli_deberta_v3.iloc[i]['scores'])) if score > 0.9\n",
    "        },\n",
    "        \"bart_large_mnli_topics\": {\n",
    "            label: score for label, score in zip(eval(df_bart_large_mnli.iloc[i]['labels']), eval(df_bart_large_mnli.iloc[i]['scores'])) if score > 0.9\n",
    "        },\n",
    "        \"flan_t5_finetuned_topics\": {\n",
    "            label: 1 for label in  eval(df_flan_t5_finetuned.iloc[i]['labels'])        \n",
    "        },\n",
    "        \"flan_t5_small_finetuned_topics\": {\n",
    "            label: 1 for label in  eval(df_flan_t5_small_finetuned.iloc[i]['labels'])        \n",
    "        },\n",
    "        \"LLM_topics\": {\n",
    "            label: 1 for label in  eval(llm.iloc[i]['labels'])\n",
    "        }   \n",
    "    }\n",
    "    reviews_topics.append(review_entry)\n",
    "\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "json_name = f'outputs/comparison_output_{ct}.json'\n",
    "# Write the dictionary to a JSON file\n",
    "with open(json_name, 'w') as json_file:\n",
    "    json.dump(reviews_topics, json_file, indent=4)\n",
    "\n",
    "print(\"Json output saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta_finetuned     Precision: 0.834  Recall: 0.720  F1: 0.773\n",
      "nli_deberta_v3        Precision: 0.461  Recall: 0.824  F1: 0.591\n",
      "bart_large_mnli       Precision: 0.603  Recall: 0.502  F1: 0.548\n",
      "flan_t5_finetuned     Precision: 0.832  Recall: 0.821  F1: 0.826\n",
      "LLM                   Precision: 0.843  Recall: 0.881  F1: 0.861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davide.zanutto1/Desktop/ratings-reviews-ai-summaries-topics/ratings-reviews/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['comfort'] will be ignored\n",
      "  warnings.warn(\n",
      "/Users/davide.zanutto1/Desktop/ratings-reviews-ai-summaries-topics/ratings-reviews/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['accentpillows', 'aesthetics', 'affordability', 'appearance', 'beauty', 'budget', 'children', 'clean', 'cleanability', 'cleanliness', 'coloroptions', 'colors', 'comfort', 'construction', 'convenience', 'couch', 'couchcushions', 'cover', 'covers', 'crafting', 'creativity', 'cushion', 'cushioncover', 'cushions', 'customersatisfaction', 'customerservice', 'customization', 'dailyuse', 'design', 'durability', 'dyeing', 'easeofinstallation', 'easeofordering', 'easeofuse', 'entertainment', 'exchange', 'faderesistance', 'features', 'feel', 'functionality', 'gift', 'gifts', 'heating', 'homedecor', 'ikea', 'lint', 'lint/doghair', 'look', 'maintenance', 'material', 'money', 'mood', 'outdooruse', 'packaging', 'pillowcover', 'pillowcovers', 'products', 'purchase', 'purchaseintent', 'quantity', 'return', 'returnpolicy', 'revamp', 'reward', 'room', 'roomtemperature', 'satisfaction', 'season', 'seasonalappeal', 'seasonalchange', 'seasonality', 'seasonaltransition', 'service', 'size', 'sizes', 'soundproofing', 'stenciling', 'style', 'support', 'sustainability', 'texture', 'transparency', 'usefulness', 'value', 'valueformoney', 'variety', 'versatility', 'vinylwork', 'wrinkling', 'zipper'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def normalize_label(label: str) -> str:\n",
    "    return label.strip().lower().replace(\" \", \"\")\n",
    "\n",
    "# 1. Load the JSON (skip the first two metadata entries)\n",
    "with open(json_name, 'r') as f:\n",
    "    data = json.load(f)\n",
    "entries = data[2:]\n",
    "\n",
    "# 2. Build a mapping from review text → prediction‐entry\n",
    "review_to_entry = { e['review']: e for e in entries }\n",
    "\n",
    "# 3. Load ground‐truth, parse lists\n",
    "df_truth = pd.read_csv('csv/ground_truth_40598766.csv')\n",
    "df_truth['topics'] = df_truth['topics'].apply(ast.literal_eval)\n",
    "\n",
    "# 4. Prepare containers\n",
    "y_true, model_preds = [], {\n",
    "    'deberta_finetuned': [],\n",
    "    'nli_deberta_v3':    [],\n",
    "    'bart_large_mnli':   [],\n",
    "    'flan_t5_finetuned': [],\n",
    "    'LLM':               []\n",
    "}\n",
    "\n",
    "# 5. Align on review text\n",
    "missing = []\n",
    "for _, row in df_truth.iterrows():\n",
    "    review = row['review']\n",
    "    if review not in review_to_entry:\n",
    "        missing.append(review)\n",
    "        continue\n",
    "    \n",
    "    entry = review_to_entry[review]\n",
    "    # normalize & collect ground truth\n",
    "    true_norm = [normalize_label(t) for t in row['topics']]\n",
    "    y_true.append(true_norm)\n",
    "    \n",
    "    # collect each model’s normalized predictions\n",
    "    for model_key, json_key in [\n",
    "        ('deberta_finetuned','deberta_finetuned_topics'),\n",
    "        ('nli_deberta_v3',   'nli_deberta_v3_topics'),\n",
    "        ('bart_large_mnli',  'bart_large_mnli_topics'),\n",
    "        ('flan_t5_finetuned','flan_t5_finetuned_topics'),\n",
    "        ('LLM','LLM_topics')\n",
    "    ]:\n",
    "        preds = list(entry[json_key].keys())\n",
    "        model_preds[model_key].append([normalize_label(t) for t in preds])\n",
    "\n",
    "if missing:\n",
    "    print(\"WARNING: no JSON entry for these reviews:\\n\", \"\\n\".join(missing))\n",
    "\n",
    "# 6. Binarize all labels against the full identified topic set\n",
    "identified = data[0]['Identified topics']\n",
    "norm_identified = [normalize_label(t) for t in identified]\n",
    "mlb = MultiLabelBinarizer(classes=norm_identified)\n",
    "y_true_bin = mlb.fit_transform(y_true)\n",
    "\n",
    "# 7. Compute metrics\n",
    "for model, pred_lists in model_preds.items():\n",
    "    y_pred_bin = mlb.transform(pred_lists)\n",
    "    p = precision_score(y_true_bin, y_pred_bin, average='micro', zero_division=0)\n",
    "    r = recall_score   (y_true_bin, y_pred_bin, average='micro', zero_division=0)\n",
    "    f = f1_score       (y_true_bin, y_pred_bin, average='micro', zero_division=0)\n",
    "    print(f\"{model:20s}  Precision: {p:.3f}  Recall: {r:.3f}  F1: {f:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: deberta_finetuned_topics\n",
      "  Precision: 0.336\n",
      "  Recall:    0.252\n",
      "  F1 Score:  0.259\n",
      "------------------------------\n",
      "Model: nli_deberta_v3_topics\n",
      "  Precision: 0.325\n",
      "  Recall:    0.470\n",
      "  F1 Score:  0.345\n",
      "------------------------------\n",
      "Model: bart_large_mnli_topics\n",
      "  Precision: 0.327\n",
      "  Recall:    0.255\n",
      "  F1 Score:  0.258\n",
      "------------------------------\n",
      "Model: flan_t5_finetuned_topics\n",
      "  Precision: 0.383\n",
      "  Recall:    0.311\n",
      "  F1 Score:  0.306\n",
      "------------------------------\n",
      "Model: LLM_topics\n",
      "  Precision: 0.288\n",
      "  Recall:    0.313\n",
      "  F1 Score:  0.270\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def load_ground_truth(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Convert the topics column (a string) into an actual list.\n",
    "    df['topics'] = df['topics'].apply(ast.literal_eval)\n",
    "    return df\n",
    "\n",
    "def load_predictions(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Filter out entries that do not contain a 'review' key\n",
    "    predictions = [entry for entry in data if \"review\" in entry]\n",
    "\n",
    "    return predictions  # List of reviews with topics\n",
    "\n",
    "def compute_precision_recall_f1(gt_topics, pred_topics):\n",
    "    # True positives: the intersection of predicted and ground truth topics.\n",
    "    true_positives = len(gt_topics & pred_topics)\n",
    "    \n",
    "    precision = true_positives / len(pred_topics) if pred_topics else 0.0\n",
    "    recall = true_positives / len(gt_topics) if gt_topics else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "def safe_literal_eval(val):\n",
    "    try:\n",
    "        if isinstance(val, str):\n",
    "            return ast.literal_eval(val)\n",
    "        return val  # If it's already a list, return as is.\n",
    "    except (ValueError, SyntaxError):\n",
    "        print(f\"Warning: Could not parse topics: {val}\")\n",
    "        return []  # Return an empty list if parsing fails.\n",
    "\n",
    "# Load the CSV file\n",
    "gt_df = pd.read_csv(\"csv/ground_truth_40598766.csv\")\n",
    "\n",
    "# Apply the safe conversion\n",
    "gt_df['topics'] = gt_df['topics'].apply(safe_literal_eval)\n",
    "\n",
    "\n",
    "predictions = load_predictions(json_name)\n",
    "\n",
    "# List the models that we want to evaluate (must match keys in the JSON)\n",
    "models = [\n",
    "    \"deberta_finetuned_topics\",\n",
    "    \"nli_deberta_v3_topics\",\n",
    "    \"bart_large_mnli_topics\",\n",
    "    \"flan_t5_finetuned_topics\",\n",
    "    \"LLM_topics\"\n",
    "]\n",
    "\n",
    "# Initialize an accumulator for metrics per model.\n",
    "metrics = {model: {\"precision\": [], \"recall\": [], \"f1\": []} for model in models}\n",
    "\n",
    "# Check that the number of reviews is the same in both files.\n",
    "if len(gt_df) != len(predictions):\n",
    "    print(\"len(gt_df),\", len(gt_df))\n",
    "    print(\"len(predictions),\", len(predictions))\n",
    "    raise ValueError(\"The number of reviews in the CSV and JSON files do not match!\")\n",
    "\n",
    "# Iterate over the reviews (assumed aligned by index)\n",
    "for idx, pred_entry in enumerate(predictions):\n",
    "    # For consistency, we compare topics in lowercase with whitespace stripped.\n",
    "    gt_topics = {topic.lower().strip() for topic in gt_df.iloc[idx][\"topics\"]}\n",
    "    \n",
    "    for model in models:\n",
    "        # In the JSON predictions, if a model has produced topics,\n",
    "        # we take the keys (ignoring the scores) as the predicted topics.\n",
    "        pred_model_dict = pred_entry.get(model, {})\n",
    "        pred_topics = {topic.lower().strip() for topic in pred_model_dict.keys()}\n",
    "        \n",
    "        precision, recall, f1 = compute_precision_recall_f1(gt_topics, pred_topics)\n",
    "        \n",
    "        metrics[model][\"precision\"].append(precision)\n",
    "        metrics[model][\"recall\"].append(recall)\n",
    "        metrics[model][\"f1\"].append(f1)\n",
    "\n",
    "# Compute the average scores for each model and print them\n",
    "for model in models:\n",
    "    avg_precision = sum(metrics[model][\"precision\"]) / len(metrics[model][\"precision\"])\n",
    "    avg_recall = sum(metrics[model][\"recall\"]) / len(metrics[model][\"recall\"])\n",
    "    avg_f1 = sum(metrics[model][\"f1\"]) / len(metrics[model][\"f1\"])\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"  Precision: {avg_precision:.3f}\")\n",
    "    print(f\"  Recall:    {avg_recall:.3f}\")\n",
    "    print(f\"  F1 Score:  {avg_f1:.3f}\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ratings-reviews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
